from typing import Dict
import streamlit as st
from core.shared_state import SharedState, AgentGraphState
from agents.moderator.agent import ModeratorAgent

def moderator_node(state: AgentGraphState, ui_callback=None, chat_container=None, log_callback=None) -> Dict:
    agent = ModeratorAgent()
    
    if ui_callback:
        ui_callback(agent.role_name, "working")
        
    # è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°
    model_name = agent.llm_config.model_name
    start_log = f"[{agent.role_name}] å¼€å§‹æ€»ç»“... (Model: {model_name})"
    if log_callback:
        log_callback(start_log)
        
    temp_state = SharedState(**state)
    
    # å‡†å¤‡æµå¼è¾“å‡º
    stream_callback = None
    placeholder = None
    accumulated_text = ""
    
    if chat_container:
        with chat_container:
            expander = st.expander(f"ğŸ—£ï¸ {agent.role_name} ({model_name})", expanded=True)
            with expander:
                placeholder = st.empty()
                
                def _callback(chunk):
                    nonlocal accumulated_text
                    accumulated_text += chunk
                    placeholder.markdown(accumulated_text + "â–Œ")
                
                stream_callback = _callback
    
    result = agent.run(temp_state, stream_callback=stream_callback)
    
    # å¤„ç†è¿”å›ç»“æœ
    if isinstance(result, dict):
        patient_reply = result.get("content", "")
        medical_summary = result.get("summary", "")
    else:
        patient_reply = result
        medical_summary = result
    
    # æ¸…é™¤å…‰æ ‡
    if placeholder:
        placeholder.markdown(accumulated_text)
    
    end_log = f"[{agent.role_name}] å®Œæˆæ€»ç»“ã€‚"
    if log_callback:
        log_callback(end_log)
        
    if ui_callback:
        ui_callback(agent.role_name, "idle")
    
    return {
        "moderator_summary": medical_summary,
        "chat_history": [
            {"role": agent.role_name, "content": patient_reply, "model": model_name}
        ],
        "agent_status": {agent.role_name: "idle"},
        "execution_logs": []
    }
